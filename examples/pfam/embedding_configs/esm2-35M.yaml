# Directory containing the input files to embed.
input_dir: data/pfam/pfam20_seed-42

# Directory to save the embeddings.
output_dir: examples/pfam/embeddings/esm2-35M_pfam20_seed-42

# Glob patterns to match the input files.
glob_patterns:
  - "*.fasta"

# Configuration for the encoder.
encoder_config:
  # The encoder name
  name: esm2
  # The model name or path to the pretrained model.
  pretrained_model_name_or_path: facebook/esm2_t12_35M_UR50D
  # Whether to use the faesm implementation (faster).
  enable_faesm: false
  # Whether to normalize the embeddings
  normalize_pooled_embeddings: true
  # The batch size to use for embedding
  dataloader_batch_size: 32

# Compute settings can be configured by referring to protein_search_evals/parsl.py
# The `name` field specifies what type of system to run on and the subsequent
# arguments are conditional on the name field (e.g., a cluster may have different
# configuration than a workstation).
compute_config:
  # Specify we want the workstation parsl configuration
  name: workstation
  # Identify which GPUs to assign tasks to. It's generally recommended to first check
  # nvidia-smi to see which GPUs are available. The numbers below are analogous to
  # setting CUDA_VISIBLE_DEVICES=1
  available_accelerators: ["1"]
